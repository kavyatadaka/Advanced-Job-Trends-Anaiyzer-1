# -*- coding: utf-8 -*-
"""Untitled23.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1wSaH0HGyiMMSM-rQlstDT4NOrhRhRhUa
"""

# %% [markdown]
"""
# Job Trends Analyzer (Enhanced)
This notebook scrapes job postings from multiple sources with both predefined and custom website options.
"""

# %% [markdown]
"""
## Setup - Run This First!
"""

# %%
# Install required packages
!pip install requests beautifulsoup4 pandas matplotlib seaborn nltk wordcloud geopy selenium webdriver-manager

# Install Chrome browser
!apt-get update
!apt-get install -y chromium-browser

# Set up ChromeDriver
!apt install -yqq chromium-chromedriver
!cp /usr/lib/chromium-browser/chromedriver /usr/bin

# Set environment variable
import os
os.environ['PATH'] += os.pathsep + '/usr/bin'

# Download NLTK data and spaCy model
import nltk
nltk.download('punkt')
nltk.download('stopwords')

try:
    import spacy
    nlp = spacy.load("en_core_web_sm")
except:
    !python -m spacy download en_core_web_sm
    import spacy
    nlp = spacy.load("en_core_web_sm")

# %% [markdown]
"""
## Import Libraries
"""

# %%
import requests
from bs4 import BeautifulSoup
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from collections import Counter
import re
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
from wordcloud import WordCloud
from geopy.geocoders import Nominatim
from geopy.extra.rate_limiter import RateLimiter
from datetime import datetime, timedelta
from selenium import webdriver
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
import time

# %% [markdown]
"""
## Web Scraper Class with Multiple Website Support
"""

# %%
class JobScraper:
    def __init__(self):
        self.geolocator = Nominatim(user_agent="job_analyzer")
        self.geocode = RateLimiter(self.geolocator.geocode, min_delay_seconds=1)
        self.driver = self._setup_selenium()

        # Predefined website configurations
        self.PREDEFINED_WEBSITES = {
            "Indeed": {
                "base_url": "https://www.indeed.com/jobs",
                "method": "requests",
                "selectors": {
                    "job_card": "div.job_seen_beacon",
                    "title": "h2.jobTitle",
                    "company": "span.companyName",
                    "location": "div.companyLocation",
                    "salary": "div.metadata.salary-snippet-container",
                    "description": "div.job-snippet",
                    "date": "span.date",
                    "link": "a.jcs-JobTitle"
                },
                "paginate": "&start={}"
            },
            "LinkedIn": {
                "base_url": "https://www.linkedin.com/jobs/search/",
                "method": "selenium",
                "selectors": {
                    "job_card": "li.jobs-search-results__list-item",
                    "title": "h3.base-search-card__title",
                    "company": "h4.base-search-card__subtitle",
                    "location": "span.job-search-card__location",
                    "description": "div.base-search-card__snippet",
                    "link": "a.base-card__full-link"
                },
                "paginate": "&start={}"
            },
            "Glassdoor": {
                "base_url": "https://www.glassdoor.com/Job/jobs.htm",
                "method": "selenium",
                "selectors": {
                    "job_card": "li.react-job-listing",
                    "title": "a.jobLink",
                    "company": "div.d-flex.justify-content-between.align-items-start",
                    "location": "span.pr-xxsm",
                    "description": "div.jobDescriptionContent",
                    "link": "a.jobLink"
                },
                "paginate": "&p={}"
            }
        }

    def _setup_selenium(self):
        """Setup Selenium WebDriver for Colab"""
        chrome_options = Options()
        chrome_options.add_argument('--headless')
        chrome_options.add_argument('--no-sandbox')
        chrome_options.add_argument('--disable-dev-shm-usage')
        chrome_options.binary_location = '/usr/bin/chromium-browser'
        return webdriver.Chrome(options=chrome_options)

    def get_job_type(self, title):
        """Classify job type based on title"""
        title_lower = title.lower()
        if 'senior' in title_lower:
            return 'Senior'
        elif 'junior' in title_lower or 'entry' in title_lower:
            return 'Junior'
        elif 'manager' in title_lower or 'lead' in title_lower:
            return 'Manager'
        elif 'director' in title_lower:
            return 'Director'
        elif 'intern' in title_lower:
            return 'Intern'
        else:
            return 'Regular'

    def parse_salary(self, salary_text):
        """Parse salary text into numerical values"""
        if not isinstance(salary_text, str) or salary_text == 'N/A':
            return None, None, None

        patterns = [
            r'\$([\d,]+)\s*-\s*\$([\d,]+)\s*a\s*year',
            r'\$([\d,]+)\s*-\s*\$([\d,]+)\s*an\s*hour',
            r'\$([\d,]+)\s*a\s*year',
            r'\$([\d,]+)\s*an\s*hour',
        ]

        for pattern in patterns:
            matches = re.findall(pattern, salary_text)
            if matches:
                if isinstance(matches[0], tuple):
                    try:
                        low = float(matches[0][0].replace(',', ''))
                        high = float(matches[0][1].replace(',', ''))
                        if 'hour' in salary_text:
                            return low, high, 'hourly'
                        else:
                            return low, high, 'yearly'
                    except:
                        continue
                else:
                    try:
                        val = float(matches[0].replace(',', ''))
                        if 'hour' in salary_text:
                            return val, None, 'hourly'
                        else:
                            return val, None, 'yearly'
                    except:
                        continue
        return None, None, None

    def parse_posted_date(self, date_text):
        """Convert relative date to actual date"""
        today = datetime.now()
        if not date_text or not isinstance(date_text, str):
            return None
        if 'today' in date_text or 'just posted' in date_text:
            return today.date()
        elif 'day' in date_text:
            days = int(re.search(r'\d+', date_text).group())
            return (today - timedelta(days=days)).date()
        elif 'month' in date_text:
            months = int(re.search(r'\d+', date_text).group())
            return (today - timedelta(days=months*30)).date()
        else:
            return None

    def get_location_coordinates(self, location_text):
        """Get latitude and longitude for location"""
        try:
            location = self.geocode(location_text)
            if location:
                return location.latitude, location.longitude
        except:
            return None, None

    def scrape_predefined_website(self, website, job_title, location, pages):
        """Scrape jobs from predefined websites"""
        if website not in self.PREDEFINED_WEBSITES:
            return []

        config = self.PREDEFINED_WEBSITES[website]
        base_url = config["base_url"]
        selectors = config["selectors"]
        paginate = config.get("paginate", "")
        method = config.get("method", "requests")

        jobs = []

        for page in range(1, pages + 1):
            if website == "LinkedIn":
                url = f"{base_url}?keywords={job_title}&location={location}{paginate.format((page-1)*25)}"
            elif website == "Indeed":
                url = f"{base_url}?q={job_title}&l={location}{paginate.format((page-1)*10)}"
            elif website == "Glassdoor":
                url = f"{base_url}?sc.keyword={job_title}&locT=C&locId=1147401{paginate.format(page)}"

            print(f"Scraping {website} page {page}: {url}")

            try:
                if method == "selenium":
                    self.driver.get(url)
                    time.sleep(3)
                    soup = BeautifulSoup(self.driver.page_source, "html.parser")
                else:
                    response = requests.get(url, headers={'User-Agent': 'Mozilla/5.0'})
                    response.raise_for_status()
                    soup = BeautifulSoup(response.text, "html.parser")

                job_cards = soup.select(selectors["job_card"])

                for card in job_cards:
                    try:
                        title = card.select_one(selectors["title"]).text.strip() if card.select_one(selectors["title"]) else "N/A"
                        company = card.select_one(selectors["company"]).text.strip() if card.select_one(selectors["company"]) else "N/A"
                        loc = card.select_one(selectors["location"]).text.strip() if card.select_one(selectors["location"]) else "N/A"
                        salary = card.select_one(selectors.get("salary", "")).text.strip() if selectors.get("salary") and card.select_one(selectors["salary"]) else "N/A"
                        desc = card.select_one(selectors["description"]).text.strip() if card.select_one(selectors["description"]) else "N/A"
                        date = card.select_one(selectors.get("date", "")).text.strip() if selectors.get("date") and card.select_one(selectors["date"]) else "N/A"

                        link_element = card.select_one(selectors["link"])
                        if link_element:
                            link = link_element.get("href", "")
                            if not link.startswith("http"):
                                if website == "LinkedIn":
                                    link = "https://www.linkedin.com" + link
                                elif website == "Indeed":
                                    link = "https://www.indeed.com" + link
                                elif website == "Glassdoor":
                                    link = "https://www.glassdoor.com" + link
                        else:
                            link = ""

                        job_type = self.get_job_type(title)
                        min_salary, max_salary, salary_type = self.parse_salary(salary)
                        posted_date = self.parse_posted_date(date)
                        lat, lon = self.get_location_coordinates(loc)

                        jobs.append({
                            "title": title,
                            "company": company,
                            "location": loc,
                            "latitude": lat,
                            "longitude": lon,
                            "salary_text": salary,
                            "min_salary": min_salary,
                            "max_salary": max_salary,
                            "salary_type": salary_type,
                            "description": desc,
                            "date_posted_text": date,
                            "date_posted": posted_date,
                            "url": link,
                            "job_type": job_type,
                            "source": website
                        })
                    except Exception as e:
                        print(f"Error processing job card: {e}")
                        continue

                time.sleep(2)  # Be polite

            except Exception as e:
                print(f"Error scraping {website} page {page}: {e}")
                continue

        return jobs

    def scrape_custom_website(self, base_url, job_title, location, pages, selectors):
        """Scrape jobs from custom websites"""
        jobs = []

        for page in range(1, pages + 1):
            # Custom URL building - this will vary by website
            url = f"{base_url}?q={job_title.replace(' ', '+')}&l={location.replace(' ', '+')}&start={(page-1)*10}"
            print(f"Scraping custom website page {page}: {url}")

            try:
                # Try both methods
                try:
                    self.driver.get(url)
                    time.sleep(3)
                    soup = BeautifulSoup(self.driver.page_source, "html.parser")
                except:
                    response = requests.get(url, headers={'User-Agent': 'Mozilla/5.0'})
                    response.raise_for_status()
                    soup = BeautifulSoup(response.text, "html.parser")

                job_cards = soup.select(selectors["job_card"])

                for card in job_cards:
                    try:
                        title = card.select_one(selectors["title"]).text.strip() if selectors.get("title") and card.select_one(selectors["title"]) else "N/A"
                        company = card.select_one(selectors["company"]).text.strip() if selectors.get("company") and card.select_one(selectors["company"]) else "N/A"
                        loc = card.select_one(selectors["location"]).text.strip() if selectors.get("location") and card.select_one(selectors["location"]) else "N/A"
                        salary = card.select_one(selectors["salary"]).text.strip() if selectors.get("salary") and card.select_one(selectors["salary"]) else "N/A"
                        desc = card.select_one(selectors["description"]).text.strip() if selectors.get("description") and card.select_one(selectors["description"]) else "N/A"
                        date = card.select_one(selectors["date"]).text.strip() if selectors.get("date") and card.select_one(selectors["date"]) else "N/A"

                        link_element = card.select_one(selectors["link"]) if selectors.get("link") else None
                        if link_element:
                            link = link_element.get("href", "")
                            if not link.startswith("http"):
                                link = base_url + link
                        else:
                            link = ""

                        job_type = self.get_job_type(title)
                        min_salary, max_salary, salary_type = self.parse_salary(salary)
                        posted_date = self.parse_posted_date(date)
                        lat, lon = self.get_location_coordinates(loc)

                        jobs.append({
                            "title": title,
                            "company": company,
                            "location": loc,
                            "latitude": lat,
                            "longitude": lon,
                            "salary_text": salary,
                            "min_salary": min_salary,
                            "max_salary": max_salary,
                            "salary_type": salary_type,
                            "description": desc,
                            "date_posted_text": date,
                            "date_posted": posted_date,
                            "url": link,
                            "job_type": job_type,
                            "source": "Custom"
                        })
                    except Exception as e:
                        print(f"Error processing job card: {e}")
                        continue

                time.sleep(2)

            except Exception as e:
                print(f"Error scraping custom website page {page}: {e}")
                continue

        return jobs

# %% [markdown]
"""
## Enhanced Job Analyzer Class
"""

# %%
class JobAnalyzer:
    def __init__(self, jobs_data):
        self.jobs_df = pd.DataFrame(jobs_data)
        self.tech_skills = [
            'python', 'java', 'javascript', 'sql', 'html', 'css', 'r',
            'machine learning', 'ai', 'deep learning', 'tensorflow',
            'pytorch', 'data analysis', 'pandas', 'numpy', 'django',
            'flask', 'react', 'angular', 'vue', 'node', 'aws',
            'azure', 'google cloud', 'docker', 'kubernetes', 'git',
            'spark', 'hadoop', 'tableau', 'power bi', 'excel',
            'nosql', 'mongodb', 'postgresql', 'mysql', 'linux'
        ]

    def clean_text(self, text):
        """Enhanced text cleaning"""
        if not isinstance(text, str):
            return ""
        text = re.sub(r'[^a-zA-Z\s]', '', text)
        text = text.lower()
        return text

    def extract_skills(self):
        """Enhanced skill extraction with spaCy NER"""
        skills = []

        for summary in self.jobs_df['description']:
            cleaned = self.clean_text(summary)

            # Basic keyword matching
            tokens = word_tokenize(cleaned)
            tokens = [word for word in tokens if word not in stopwords.words('english')]
            found_skills = [skill for skill in self.tech_skills if skill in ' '.join(tokens)]
            skills.extend(found_skills)

            # spaCy NER for skills
            doc = nlp(cleaned)
            for ent in doc.ents:
                if ent.label_ in ['ORG', 'PRODUCT', 'TECH']:
                    skills.append(ent.text.lower())

        return skills

    def analyze_salaries(self):
        """Analyze salary trends by job type and location"""
        if 'min_salary' not in self.jobs_df.columns:
            return None

        # Convert hourly to yearly (assuming 2000 work hours/year)
        df = self.jobs_df.copy()
        df['salary_estimate'] = df.apply(lambda x:
            (x['min_salary'] + (x['max_salary'] or x['min_salary'])) / 2 * 2000
            if x['salary_type'] == 'hourly' else
            (x['min_salary'] + (x['max_salary'] or x['min_salary'])) / 2,
            axis=1)

        # Filter out unrealistic salaries
        df = df[(df['salary_estimate'] > 10000) & (df['salary_estimate'] < 500000)]

        return df

    def analyze_trends(self):
        """Enhanced trend analysis with more metrics"""
        print("\n=== Enhanced Job Trends Analysis ===\n")

        # 1. Job titles analysis
        print("Top 10 Job Titles:")
        title_counts = self.jobs_df['title'].value_counts().head(10)
        print(title_counts)

        # 2. Companies analysis
        print("\nTop 10 Companies Hiring:")
        company_counts = self.jobs_df['company'].value_counts().head(10)
        print(company_counts)

        # 3. Locations analysis
        print("\nTop 10 Locations:")
        location_counts = self.jobs_df['location'].value_counts().head(10)
        print(location_counts)

        # 4. Skills analysis
        skills = self.extract_skills()
        print("\nTop 10 Skills in Demand:")
        skill_counts = Counter(skills).most_common(10)
        for skill, count in skill_counts:
            print(f"{skill}: {count}")

        # 5. Job type distribution
        if 'job_type' in self.jobs_df.columns:
            print("\nJob Type Distribution:")
            job_type_counts = self.jobs_df['job_type'].value_counts()
            print(job_type_counts)

        # 6. Salary analysis
        salary_df = self.analyze_salaries()
        if salary_df is not None and not salary_df.empty:
            print("\nSalary Statistics:")
            print(f"Average salary: ${salary_df['salary_estimate'].mean():,.2f}")
            print(f"Median salary: ${salary_df['salary_estimate'].median():,.2f}")
            print(f"Highest salary: ${salary_df['salary_estimate'].max():,.2f}")
            print(f"Lowest salary: ${salary_df['salary_estimate'].min():,.2f}")

        # 7. Source distribution
        if 'source' in self.jobs_df.columns:
            print("\nJob Source Distribution:")
            source_counts = self.jobs_df['source'].value_counts()
            print(source_counts)

        return {
            'title_counts': title_counts,
            'company_counts': company_counts,
            'location_counts': location_counts,
            'skill_counts': skill_counts,
            'job_type_counts': job_type_counts if 'job_type' in self.jobs_df.columns else None,
            'salary_df': salary_df,
            'source_counts': source_counts if 'source' in self.jobs_df.columns else None
        }

    def visualize_trends(self, trends):
        """Enhanced visualization with more charts"""
        plt.figure(figsize=(20, 15))

        # 1. Job titles
        plt.subplot(3, 3, 1)
        trends['title_counts'].plot(kind='barh', color='skyblue')
        plt.title('Top Job Titles')
        plt.xlabel('Count')

        # 2. Companies
        plt.subplot(3, 3, 2)
        trends['company_counts'].plot(kind='barh', color='lightgreen')
        plt.title('Top Companies Hiring')
        plt.xlabel('Count')

        # 3. Locations
        plt.subplot(3, 3, 3)
        trends['location_counts'].plot(kind='barh', color='salmon')
        plt.title('Top Locations')
        plt.xlabel('Count')

        # 4. Skills
        plt.subplot(3, 3, 4)
        skills, counts = zip(*trends['skill_counts'])
        plt.barh(skills, counts, color='purple')
        plt.title('Top Skills in Demand')
        plt.xlabel('Count')

        # 5. Job types
        if trends['job_type_counts'] is not None:
            plt.subplot(3, 3, 5)
            trends['job_type_counts'].plot(kind='pie', autopct='%1.1f%%', colors=['gold', 'lightcoral', 'lightskyblue', 'lightgreen'])
            plt.title('Job Type Distribution')
            plt.ylabel('')

        # 6. Salary distribution
        if trends['salary_df'] is not None and not trends['salary_df'].empty:
            plt.subplot(3, 3, 6)
            sns.boxplot(data=trends['salary_df'], y='salary_estimate', x='job_type')
            plt.title('Salary Distribution by Job Type')
            plt.ylabel('Salary ($)')
            plt.xlabel('Job Type')
            plt.xticks(rotation=45)

        # 7. Source distribution
        if trends['source_counts'] is not None:
            plt.subplot(3, 3, 7)
            trends['source_counts'].plot(kind='pie', autopct='%1.1f%%', colors=['lightblue', 'lightgreen', 'pink', 'lightyellow'])
            plt.title('Job Source Distribution')
            plt.ylabel('')

        plt.tight_layout()
        plt.show()

        # Word cloud of job descriptions
        all_descriptions = ' '.join(self.jobs_df['description'].dropna())
        wordcloud = WordCloud(width=800, height=400, background_color='white').generate(all_descriptions)

        plt.figure(figsize=(10, 5))
        plt.imshow(wordcloud, interpolation='bilinear')
        plt.axis('off')
        plt.title('Word Cloud of Job Descriptions')
        plt.show()

        # Location heatmap if coordinates available
        if 'latitude' in self.jobs_df.columns and 'longitude' in self.jobs_df.columns:
            location_df = self.jobs_df.dropna(subset=['latitude', 'longitude'])
            if not location_df.empty:
                plt.figure(figsize=(10, 6))
                plt.scatter(location_df['longitude'], location_df['latitude'], alpha=0.5, c='blue')
                plt.title('Job Locations')
                plt.xlabel('Longitude')
                plt.ylabel('Latitude')
                plt.grid(True)
                plt.show()

    def save_analysis_report(self, filename="job_analysis_report.txt"):
        """Save analysis report to text file"""
        trends = self.analyze_trends()

        with open(filename, 'w') as f:
            f.write("=== Job Analysis Report ===\n\n")
            f.write(f"Generated on: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n\n")

            f.write("Top Job Titles:\n")
            for title, count in trends['title_counts'].items():
                f.write(f"{title}: {count}\n")

            f.write("\nTop Companies Hiring:\n")
            for company, count in trends['company_counts'].items():
                f.write(f"{company}: {count}\n")

            f.write("\nTop Locations:\n")
            for location, count in trends['location_counts'].items():
                f.write(f"{location}: {count}\n")

            f.write("\nTop Skills in Demand:\n")
            for skill, count in trends['skill_counts']:
                f.write(f"{skill}: {count}\n")

            if trends['job_type_counts'] is not None:
                f.write("\nJob Type Distribution:\n")
                for job_type, count in trends['job_type_counts'].items():
                    f.write(f"{job_type}: {count}\n")

            if trends['salary_df'] is not None and not trends['salary_df'].empty:
                f.write("\nSalary Statistics:\n")
                f.write(f"Average salary: ${trends['salary_df']['salary_estimate'].mean():,.2f}\n")
                f.write(f"Median salary: ${trends['salary_df']['salary_estimate'].median():,.2f}\n")
                f.write(f"Highest salary: ${trends['salary_df']['salary_estimate'].max():,.2f}\n")
                f.write(f"Lowest salary: ${trends['salary_df']['salary_estimate'].min():,.2f}\n")

            if trends['source_counts'] is not None:
                f.write("\nJob Source Distribution:\n")
                for source, count in trends['source_counts'].items():
                    f.write(f"{source}: {count}\n")

        print(f"Analysis report saved to {filename}")

# %% [markdown]
"""
## Main Execution with Both Predefined and Custom Options
"""

# %%
def main():
    # Initialize scraper
    scraper = JobScraper()

    print("=== Job Trends Analyzer ===")
    print("\nChoose scraping option:")
    print("1. Use predefined websites (Indeed, LinkedIn, Glassdoor)")
    print("2. Use custom website")

    while True:
        choice = input("\nEnter your choice (1 or 2): ")
        if choice in ('1', '2'):
            break
        print("Invalid input. Please enter 1 or 2.")

    job_title = input("\nEnter job title to search (e.g., 'Data Scientist'): ").strip() or "Data Scientist"
    location = input("Enter location (e.g., 'New York'): ").strip() or "United States"

    while True:
        try:
            pages = int(input("\nEnter number of pages to scrape (1 page â‰ˆ 10 jobs, max 10): ").strip() or 1)
            if 1 <= pages <= 10:
                break
            print("Please enter a number between 1 and 10.")
        except ValueError:
            print("Invalid input. Please enter a number.")

    save_report = input("\nSave analysis report? (y/n): ").strip().lower() == 'y'

    all_jobs = []

    if choice == '1':
        print("\nAvailable predefined websites:")
        websites = list(scraper.PREDEFINED_WEBSITES.keys())
        for i, site in enumerate(websites, 1):
            print(f"{i}. {site}")

        while True:
            selected = input("\nEnter websites to scrape (comma separated numbers, or 'all'): ").strip().lower()
            if selected == 'all':
                selected_websites = websites
                break
            else:
                try:
                    selected_indices = [int(i.strip()) for i in selected.split(',')]
                    selected_websites = [websites[i-1] for i in selected_indices if 1 <= i <= len(websites)]
                    if selected_websites:
                        break
                    print("No valid websites selected.")
                except:
                    print("Invalid input. Please enter numbers separated by commas or 'all'.")

        print(f"\nScraping {', '.join(selected_websites)} for '{job_title}' jobs in '{location}'...")

        for website in selected_websites:
            print(f"\nScraping {website}...")
            jobs = scraper.scrape_predefined_website(website, job_title, location, pages)
            all_jobs.extend(jobs)
            print(f"Found {len(jobs)} jobs from {website}")

    elif choice == '2':
        while True:
            base_url = input("\nEnter base URL of job website (e.g., https://www.example.com/jobs): ").strip()
            if base_url.startswith(('http://', 'https://')):
                break
            print("Invalid URL. Please include http:// or https://")

        print("\nPlease enter CSS selectors for the following elements:")
        print("(Leave blank for elements you don't want to scrape)")

        selectors = {
            'job_card': input("\nJob card selector (e.g., div.job-card): ").strip(),
            'title': input("Job title selector (e.g., h2.job-title): ").strip(),
            'company': input("Company name selector (e.g., span.company): ").strip(),
            'location': input("Location selector (e.g., div.location): ").strip(),
            'salary': input("Salary selector (optional): ").strip(),
            'description': input("Description selector (e.g., div.description): ").strip(),
            'date': input("Posted date selector (optional): ").strip(),
            'link': input("Job link selector (e.g., a.job-link): ").strip()
        }

        # Remove empty selectors
        selectors = {k: v for k, v in selectors.items() if v}

        if not selectors.get('job_card') or not selectors.get('title'):
            print("\nError: At least job card and title selectors are required.")
            return

        print(f"\nScraping custom website for '{job_title}' jobs in '{location}'...")
        jobs = scraper.scrape_custom_website(base_url, job_title, location, pages, selectors)
        all_jobs.extend(jobs)
        print(f"\nFound {len(jobs)} jobs from custom website")

    if not all_jobs:
        print("\nNo jobs found. Try different parameters.")
        return

    print(f"\nTotal jobs collected: {len(all_jobs)}")

    # Analyze trends
    analyzer = JobAnalyzer(all_jobs)
    trends = analyzer.analyze_trends()

    # Visualize trends
    analyzer.visualize_trends(trends)

    # Save results
    df = pd.DataFrame(all_jobs)
    df.to_csv('job_postings.csv', index=False)
    print("\nJob postings saved to 'job_postings.csv'")

    if save_report:
        analyzer.save_analysis_report()

# Run the main function
if __name__ == "__main__":
    main()

